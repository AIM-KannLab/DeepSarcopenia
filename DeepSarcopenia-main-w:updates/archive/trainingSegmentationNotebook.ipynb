{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640c3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from scripts.segmentation import train\n",
    "from scripts import data_generator\n",
    "import os\n",
    "import warnings\n",
    "import scipy.integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254ca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#\n",
    " #   parser = argparse.ArgumentParser(description  =  'Train a u-net model on multiple classes')\n",
    "  #  parser.add_argument('--data_dir', '-d',type = str, default = '../data/train/train_segmentation/', \n",
    "   #                     help  =  'Directory in which the segmentation training data arrays are stored')\n",
    "    #parser.add_argument('--model_dir', '-m',type = str, default = '../model/train/unet_models/', \n",
    "   #                     help = 'Location where trained models are to be stored')\n",
    "    \n",
    " #   parser.add_argument('--epochs', '-e', type  =  int, default  =  600, help = 'number of training epochs')\n",
    " #   parser.add_argument('--batch_size','-b', type  =  int, default  =  1, help = 'batch size')\n",
    " #   parser.add_argument('--load_weights','-w', help  =  'load weights in this file to initialise model')\n",
    " #   parser.add_argument('--name','-a', help  =  'trained model will be stored in a directory with this name')\n",
    " #   parser.add_argument('--gpus','-g', type  =  int, default  =  1, help  =  'number of gpus')\n",
    " #   parser.add_argument('--learning_rate','-l', type = float, default = 0.004, help = 'learning rate')\n",
    " #   parser.add_argument('--upsamlping_modules','-D', type = int,  default = 5,\n",
    " #                       help = 'downsampling/upsamlping module numbers')\n",
    " #   parser.add_argument('--initial_features','-F', type = int, default = 16,\n",
    " #                       help = 'number of feautres in first model')\n",
    " #   parser.add_argument('--activation','-A', default = 'relu', help = 'activation function to use')\n",
    " #   parser.add_argument('--num_convs','-N', type = int, default = 2, help = 'activation function to use')\n",
    "    \n",
    "#    args = parser.parse_args()\n",
    "    \n",
    " #   model = train(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb14c841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "images_train.shape,images_val.shape (345, 512, 512, 1) (86, 512, 512, 1) \n",
      "\n",
      "\n",
      "\n",
      "Creating and compiling model...\n",
      "\n",
      " \n",
      "  val_batches:::: 86 \n",
      "\n",
      "\n",
      " \n",
      "  train_batches:::: 345 \n",
      "\n",
      "Fitting model...\n",
      "                                       Learning Rate Update at Epoch 0\n",
      "Lr 0.0005\n",
      "Epoch 1/100\n",
      "345/345 [==============================] - 73s 204ms/step - loss: 0.3574 - val_loss: 0.4751 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 1\n",
      "Lr 0.0005\n",
      "Epoch 2/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: 0.1709 - val_loss: 0.1526 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 2\n",
      "Lr 0.0005\n",
      "Epoch 3/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: 0.1225 - val_loss: 0.3152 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 3\n",
      "Lr 0.0005\n",
      "Epoch 4/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: 0.1032 - val_loss: 0.4362 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 4\n",
      "Lr 0.0005\n",
      "Epoch 5/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: 0.0935 - val_loss: 0.9581 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 5\n",
      "Lr 0.0005\n",
      "Epoch 6/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.6371 - val_loss: -0.6763 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 6\n",
      "Lr 0.0005\n",
      "Epoch 7/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9114 - val_loss: -0.8645 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 7\n",
      "Lr 0.0005\n",
      "Epoch 8/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9184 - val_loss: -0.8655 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 8\n",
      "Lr 0.0005\n",
      "Epoch 9/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9162 - val_loss: -0.8043 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 9\n",
      "Lr 0.0005\n",
      "Epoch 10/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9207 - val_loss: -0.8281 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 10\n",
      "Lr 0.0005\n",
      "Epoch 11/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9219 - val_loss: -0.6359 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 11\n",
      "Lr 0.0005\n",
      "Epoch 12/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9246 - val_loss: -0.7525 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 12\n",
      "Lr 0.0005\n",
      "Epoch 13/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9216 - val_loss: -0.8932 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 13\n",
      "Lr 0.0005\n",
      "Epoch 14/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9270 - val_loss: -0.2585 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 14\n",
      "Lr 0.0005\n",
      "Epoch 15/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9314 - val_loss: -0.8998 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 15\n",
      "Lr 0.0005\n",
      "Epoch 16/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9103 - val_loss: -0.6089 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 16\n",
      "Lr 0.0005\n",
      "Epoch 17/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9246 - val_loss: -0.8575 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 17\n",
      "Lr 0.0005\n",
      "Epoch 18/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9317 - val_loss: -0.8759 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 18\n",
      "Lr 0.0005\n",
      "Epoch 19/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9334 - val_loss: -0.8313 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 19\n",
      "Lr 0.0005\n",
      "Epoch 20/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9321 - val_loss: -0.6653 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 20\n",
      "Lr 0.0005\n",
      "Epoch 21/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9351 - val_loss: -0.8049 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 21\n",
      "Lr 0.0005\n",
      "Epoch 22/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9365 - val_loss: -0.8511 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 22\n",
      "Lr 0.0005\n",
      "Epoch 23/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9370 - val_loss: -0.8565 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 23\n",
      "Lr 0.0005\n",
      "Epoch 24/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9351 - val_loss: -0.5091 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 24\n",
      "Lr 0.0005\n",
      "Epoch 25/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9371 - val_loss: -0.8947 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 25\n",
      "Lr 0.0005\n",
      "Epoch 26/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9366 - val_loss: -0.9094 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 26\n",
      "Lr 0.0005\n",
      "Epoch 27/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9363 - val_loss: -0.9101 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 27\n",
      "Lr 0.0005\n",
      "Epoch 28/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9396 - val_loss: -0.8832 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 28\n",
      "Lr 0.0005\n",
      "Epoch 29/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9397 - val_loss: -0.9137 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 29\n",
      "Lr 0.0005\n",
      "Epoch 30/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9400 - val_loss: -0.9144 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 30\n",
      "Lr 0.0005\n",
      "Epoch 31/100\n",
      "345/345 [==============================] - 71s 206ms/step - loss: -0.9389 - val_loss: -0.8023 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 31\n",
      "Lr 0.0005\n",
      "Epoch 32/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9412 - val_loss: -0.8672 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 32\n",
      "Lr 0.0005\n",
      "Epoch 33/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9414 - val_loss: -0.6724 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 33\n",
      "Lr 0.0005\n",
      "Epoch 34/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9421 - val_loss: -0.7568 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 34\n",
      "Lr 0.0005\n",
      "Epoch 35/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9430 - val_loss: -0.6445 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 35\n",
      "Lr 0.0005\n",
      "Epoch 36/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9433 - val_loss: -0.3300 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 36\n",
      "Lr 0.0005\n",
      "Epoch 37/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9427 - val_loss: -0.7490 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 37\n",
      "Lr 0.0005\n",
      "Epoch 38/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9442 - val_loss: -0.8497 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 38\n",
      "Lr 0.0005\n",
      "Epoch 39/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9440 - val_loss: -0.8672 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 39\n",
      "Lr 0.0005\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9431 - val_loss: -0.8396 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 40\n",
      "Lr 0.0005\n",
      "Epoch 41/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9457 - val_loss: -0.6670 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 41\n",
      "Lr 0.0005\n",
      "Epoch 42/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9424 - val_loss: -0.7043 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 42\n",
      "Lr 0.0005\n",
      "Epoch 43/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9447 - val_loss: -0.6795 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 43\n",
      "Lr 0.0005\n",
      "Epoch 44/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9457 - val_loss: -0.8573 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 44\n",
      "Lr 0.0005\n",
      "Epoch 45/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9466 - val_loss: -0.8034 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 45\n",
      "Lr 0.0005\n",
      "Epoch 46/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9466 - val_loss: -0.8634 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 46\n",
      "Lr 0.0005\n",
      "Epoch 47/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9462 - val_loss: -0.8009 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 47\n",
      "Lr 0.0005\n",
      "Epoch 48/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9470 - val_loss: -0.7269 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 48\n",
      "Lr 0.0005\n",
      "Epoch 49/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9479 - val_loss: -0.8895 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 49\n",
      "Lr 0.0005\n",
      "Epoch 50/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9480 - val_loss: -0.7701 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 50\n",
      "Lr 0.0005\n",
      "Epoch 51/100\n",
      "345/345 [==============================] - 71s 204ms/step - loss: -0.9464 - val_loss: -0.5859 - lr: 5.0000e-04\n",
      "                                       Learning Rate Update at Epoch 51\n",
      "                             Lr  0.00025\n",
      "Epoch 52/100\n",
      "345/345 [==============================] - 71s 204ms/step - loss: -0.9511 - val_loss: -0.8257 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 52\n",
      "                             Lr  0.00025\n",
      "Epoch 53/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9518 - val_loss: -0.8261 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 53\n",
      "                             Lr  0.00025\n",
      "Epoch 54/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9522 - val_loss: -0.8191 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 54\n",
      "                             Lr  0.00025\n",
      "Epoch 55/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9526 - val_loss: -0.8345 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 55\n",
      "                             Lr  0.00025\n",
      "Epoch 56/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9527 - val_loss: -0.8437 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 56\n",
      "                             Lr  0.00025\n",
      "Epoch 57/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9527 - val_loss: -0.8295 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 57\n",
      "                             Lr  0.00025\n",
      "Epoch 58/100\n",
      "345/345 [==============================] - 71s 204ms/step - loss: -0.9529 - val_loss: -0.8476 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 58\n",
      "                             Lr  0.00025\n",
      "Epoch 59/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9532 - val_loss: -0.8236 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 59\n",
      "                             Lr  0.00025\n",
      "Epoch 60/100\n",
      "345/345 [==============================] - 71s 206ms/step - loss: -0.9534 - val_loss: -0.8390 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 60\n",
      "                             Lr  0.00025\n",
      "Epoch 61/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9536 - val_loss: -0.8066 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 61\n",
      "                             Lr  0.00025\n",
      "Epoch 62/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9537 - val_loss: -0.8365 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 62\n",
      "                             Lr  0.00025\n",
      "Epoch 63/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9539 - val_loss: -0.8208 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 63\n",
      "                             Lr  0.00025\n",
      "Epoch 64/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9539 - val_loss: -0.8156 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 64\n",
      "                             Lr  0.00025\n",
      "Epoch 65/100\n",
      "345/345 [==============================] - 71s 206ms/step - loss: -0.9541 - val_loss: -0.8343 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 65\n",
      "                             Lr  0.00025\n",
      "Epoch 66/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9541 - val_loss: -0.7888 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 66\n",
      "                             Lr  0.00025\n",
      "Epoch 67/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9542 - val_loss: -0.8185 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 67\n",
      "                             Lr  0.00025\n",
      "Epoch 68/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9544 - val_loss: -0.8108 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 68\n",
      "                             Lr  0.00025\n",
      "Epoch 69/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9545 - val_loss: -0.8190 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 69\n",
      "                             Lr  0.00025\n",
      "Epoch 70/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9546 - val_loss: -0.8226 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 70\n",
      "                             Lr  0.00025\n",
      "Epoch 71/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9546 - val_loss: -0.7994 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 71\n",
      "                             Lr  0.00025\n",
      "Epoch 72/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9549 - val_loss: -0.8249 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 72\n",
      "                             Lr  0.00025\n",
      "Epoch 73/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9549 - val_loss: -0.8317 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 73\n",
      "                             Lr  0.00025\n",
      "Epoch 74/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9550 - val_loss: -0.8098 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 74\n",
      "                             Lr  0.00025\n",
      "Epoch 75/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9551 - val_loss: -0.8101 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 75\n",
      "                             Lr  0.00025\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9550 - val_loss: -0.8309 - lr: 5.0000e-05\n",
      "                                       Learning Rate Update at Epoch 76\n",
      "                                 Lr  0.0001\n",
      "Epoch 77/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9556 - val_loss: -0.8119 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 77\n",
      "                                 Lr  0.0001\n",
      "Epoch 78/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9557 - val_loss: -0.8239 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 78\n",
      "                                 Lr  0.0001\n",
      "Epoch 79/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9557 - val_loss: -0.8150 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 79\n",
      "                                 Lr  0.0001\n",
      "Epoch 80/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9557 - val_loss: -0.8226 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 80\n",
      "                                 Lr  0.0001\n",
      "Epoch 81/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9558 - val_loss: -0.8107 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 81\n",
      "                                 Lr  0.0001\n",
      "Epoch 82/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9557 - val_loss: -0.8164 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 82\n",
      "                                 Lr  0.0001\n",
      "Epoch 83/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9557 - val_loss: -0.8204 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 83\n",
      "                                 Lr  0.0001\n",
      "Epoch 84/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9558 - val_loss: -0.8075 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 84\n",
      "                                 Lr  0.0001\n",
      "Epoch 85/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9559 - val_loss: -0.8181 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 85\n",
      "                                 Lr  0.0001\n",
      "Epoch 86/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9558 - val_loss: -0.8121 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 86\n",
      "                                 Lr  0.0001\n",
      "Epoch 87/100\n",
      "345/345 [==============================] - 71s 205ms/step - loss: -0.9560 - val_loss: -0.8150 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 87\n",
      "                                 Lr  0.0001\n",
      "Epoch 88/100\n",
      "345/345 [==============================] - 70s 204ms/step - loss: -0.9559 - val_loss: -0.8133 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 88\n",
      "                                 Lr  0.0001\n",
      "Epoch 89/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9559 - val_loss: -0.8152 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 89\n",
      "                                 Lr  0.0001\n",
      "Epoch 90/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9560 - val_loss: -0.8031 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 90\n",
      "                                 Lr  0.0001\n",
      "Epoch 91/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9559 - val_loss: -0.8118 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 91\n",
      "                                 Lr  0.0001\n",
      "Epoch 92/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9559 - val_loss: -0.7989 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 92\n",
      "                                 Lr  0.0001\n",
      "Epoch 93/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9559 - val_loss: -0.8150 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 93\n",
      "                                 Lr  0.0001\n",
      "Epoch 94/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9559 - val_loss: -0.8136 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 94\n",
      "                                 Lr  0.0001\n",
      "Epoch 95/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9561 - val_loss: -0.8113 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 95\n",
      "                                 Lr  0.0001\n",
      "Epoch 96/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9560 - val_loss: -0.7989 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 96\n",
      "                                 Lr  0.0001\n",
      "Epoch 97/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9561 - val_loss: -0.8197 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 97\n",
      "                                 Lr  0.0001\n",
      "Epoch 98/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9561 - val_loss: -0.8215 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 98\n",
      "                                 Lr  0.0001\n",
      "Epoch 99/100\n",
      "345/345 [==============================] - 70s 202ms/step - loss: -0.9562 - val_loss: -0.8161 - lr: 5.0000e-06\n",
      "                                       Learning Rate Update at Epoch 99\n",
      "                                 Lr  0.0001\n",
      "Epoch 100/100\n",
      "345/345 [==============================] - 70s 203ms/step - loss: -0.9562 - val_loss: -0.8136 - lr: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "model=train('../data/train/train_segmentation/','../model/train/unet_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce539dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
